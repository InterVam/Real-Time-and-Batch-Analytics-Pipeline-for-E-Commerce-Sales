# ğŸ“Š Data Engineering Project: Real-Time Streaming Pipeline with Kafka & PySpark

## âœ… Overview
This project demonstrates an end-to-end streaming data pipeline using:
- **Kafka** for simulating real-time sales transactions.
- **PySpark Structured Streaming** for processing, aggregating, and saving results.
- **Local Parquet output** for batch analytics and future BI integration.

---

## âœ… Architecture Summary
- **Kafka (via Docker)**: Receives simulated sales transactions.
- **PySpark (Jupyter Notebook Docker)**: 
  - Connects to Kafka.
  - Consumes streaming data.
  - Aggregates sales by product category.
  - Saves results continuously as Parquet files.
- **Output**: Local Parquet files in a `/streaming_sales_output/` directory with checkpoints for fault tolerance.

---

## âœ… Folder Structure
```
project-root/
â”œâ”€â”€ Kafka/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ create-kafka-topic.sh
â”œâ”€â”€ pyspark/
â”‚   â”œâ”€â”€ spark_streaming_job.py
â”‚   â””â”€â”€ streaming_sales_output/ (generated)
â”‚       â”œâ”€â”€ part-*.parquet
â”‚       â””â”€â”€ checkpoints/
â””â”€â”€ Python-Sales-Simulation-Script/
    â”œâ”€â”€ sales_simulation_producer.py
    â”œâ”€â”€ generate_customer_profiles.py
    â””â”€â”€ customer_profiles.csv
```

---

## âœ… How to Run the Pipeline

### 1ï¸âƒ£ Start Kafka & Zookeeper
```bash
cd Kafka
docker-compose up -d
```

### 2ï¸âƒ£ Verify Kafka topic
```bash
docker exec -it <kafka_container_id> bash
kafka-topics --list --bootstrap-server localhost:9092
```
If the `sales_transactions` topic doesnâ€™t exist, create it:
```bash
kafka-topics --create --topic sales_transactions --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

### 3ï¸âƒ£ Start PySpark Jupyter Notebook
```bash
docker run -it --rm \
  -p 8888:8888 \
  -v $(pwd)/pyspark:/home/jovyan/work \
  --network data-pipeline-net \
  jupyter/pyspark-notebook
```

### 4ï¸âƒ£ Run the `spark_streaming_job.py` from the notebook or as a script
- The script:
  - Connects to Kafka.
  - Aggregates sales transactions by product category in 1-minute windows.
  - Writes output continuously to Parquet.

### 5ï¸âƒ£ Start the Kafka producer simulation
```bash
cd Python-Sales-Simulation-Script
python sales_simulation_producer.py
```

### 6ï¸âƒ£ Check Parquet output
- Parquet files will appear in:
```
pyspark/streaming_sales_output/
```

---

## âœ… Next Steps
- Add batch job processing for `customer_profiles.csv`.
- Visualize aggregated Parquet results in a Jupyter notebook.
- Optionally deploy on Azure Blob or Databricks.

---

## âœ… Contact
Made by Youssef â€” feel free to connect or reach out for questions!
