# ğŸ“Š Batch & Streaming Data Pipeline Project

## ğŸš€ Overview
This project demonstrates a full end-to-end data engineering pipeline featuring both real-time streaming data ingestion and batch data processing. The pipeline simulates e-commerce transactions and customer profile data to build a scalable analytics solution.

---

## âœ… Architecture
- **Streaming Ingestion**: Kafka simulates real-time sales transactions.
- **Batch Ingestion**: Historical customer profile CSV stored locally (can be extended to S3 or Azure Blob).
- **Processing Layer**:
  - Spark Structured Streaming for real-time processing.
  - PySpark batch jobs for historical data.
- **Storage**: Simulated data lake via local directories or cloud integration.
- **Orchestration**: Airflow (optional for scheduling batch jobs).
- **Visualization**: Connect to Power BI or Looker for analytics (optional).

---

## ğŸ“‚ Folder Structure
```
â”œâ”€â”€ Kafka
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ create-kafka-topic.sh
â”‚
â”œâ”€â”€ Python-Sales-Simulation-Script
â”‚   â”œâ”€â”€ generate_customer_profiles.py
â”‚   â”œâ”€â”€ sales_simulation_producer.py
â”‚   â”œâ”€â”€ customer_profiles.csv (generated)
â”‚   â””â”€â”€ customer_ids.json (generated)
â”‚
â””â”€â”€ pyspark
    â”œâ”€â”€ spark_batch_job.py (to be added)
    â””â”€â”€ spark_streaming_job.py (to be added)
```

---

## ğŸ”§ Prerequisites
- Docker & Docker Compose
- Python 3.8+
- Kafka (via Docker)
- PySpark
- pandas, faker, kafka-python (Python libraries)

---

## ğŸ›  Setup Instructions

### 1. Kafka Setup
```bash
cd Kafka
docker-compose up --build -d
```

### 2. Generate Historical Customer Profiles
```bash
cd Python-Sales-Simulation-Script
python generate_customer_profiles.py
```
This creates `customer_profiles.csv` and `customer_ids.json`.

### 3. Start Sales Transaction Producer
```bash
python sales_simulation_producer.py
```
This will send random transactions to Kafka.

### 4. (Optional) Consume messages for testing
```bash
docker exec -it <kafka_container_id> bash
kafka-console-consumer --topic sales_transactions --bootstrap-server kafka:9092 --from-beginning
```

---

## ğŸ” Next Steps (Coming Soon)
- Add Spark streaming job to consume from Kafka and write Parquet outputs.
- Add Spark batch ETL to transform `customer_profiles.csv`.
- Connect Spark output to Snowflake or Databricks.
- Visualize aggregated metrics in Power BI.

---

## ğŸ¤ Contributions
PRs are welcome! Feel free to fork the repo and add enhancements.

---

## ğŸ“« Contact
Created by Youssef â€” [LinkedIn](https://www.linkedin.com/in/yfathi2000/) | [GitHub](https://github.com/InterVam)

