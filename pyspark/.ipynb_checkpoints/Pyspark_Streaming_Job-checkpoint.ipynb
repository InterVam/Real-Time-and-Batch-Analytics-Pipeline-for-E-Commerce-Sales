{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cedee-8483-4d71-ace4-7d7a32ba50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "\n",
    "# Initialize Spark session with Kafka connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSalesStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"kafka-broker:9092\"  # For Docker-to-Docker networking\n",
    "KAFKA_TOPIC = \"sales_transactions\"\n",
    "\n",
    "# Define schema for incoming JSON\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"product_category\", StringType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"payment_method\", StringType())\n",
    "])\n",
    "\n",
    "print(\"✅ Starting to read stream from Kafka...\")\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"✅ Kafka stream connected. Parsing incoming data...\")\n",
    "\n",
    "# Parse JSON messages\n",
    "parsed_df = stream_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Convert timestamp field\n",
    "parsed_df = parsed_df.withColumn(\"event_time\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "print(\"✅ Streaming data parsed. Aggregating sales by category...\")\n",
    "\n",
    "# Aggregate sales by product category in 1-minute windows\n",
    "aggregated_df = parsed_df \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\"),\n",
    "        col(\"product_category\")\n",
    "    ).agg(\n",
    "        spark_sum(\"amount\").alias(\"total_sales\")\n",
    "    )\n",
    "\n",
    "print(\"✅ Setting up streaming query to write directly to parquet...\")\n",
    "\n",
    "# Start the query: write directly to Parquet files\n",
    "query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/home/jovyan/work/streaming_sales_output/\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/work/streaming_sales_output/checkpoints/\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ Query started. Awaiting streaming results...\")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
